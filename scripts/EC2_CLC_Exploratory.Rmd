---
title: "CLC"
author: "AMP"
date: "`r Sys.Date()`"
output: html_document
---

From Matt K

Linked below are two GIS datasets to use in the regression analysis. I’d like to try vertical retreat rate vs. soil parameters, and threshold elevations vs. soil parameters.
 
The first dataset is vertical forest retreat rates for every point along the mid-Atlantic. 
#### Just make sure to use the one labeled “vertical forest retreat” rather than “lateral forest retreat.”
The regression analysis should be straight forward since both datasets are point based. https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-vcr.399.1
 
The second dataset is the threshold elevation defining the boundary between forest and marsh, aggregated by watershed. This is only for Maryland and Virginia, so won’t work for EXCHANGE sites in DE or NJ.
https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-vcr.343.2
File #1 contains the watershed names and threshold elevations. File #3 contains the watershed boundaries. You’ll need to identify the watershed that the EXCHANGE site is in, then find the threshold elevation for that watershed before running the regression. Its simpler than it sounds.
 
I’m sending links rather than the actual files so that you have all the metadata and other potentially useful files handy.


```{r setup, include=FALSE}
library(tidyverse)
library(sf)
library(raster)
library(corrr)
library(ranger)
library(tidymodels)
#library(rgdal)

getwd()
```

# Vertical Retreat Rates 
## Load in Data

#### vertical forest retreat rates from: 

Chen, Y. and M.L. Kirwan. 2023. vertical and vertical forest retreat rate in the mid-Atlantic sea-level rise hotspot ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/4edf9b0d9d6660d354710748b2cf56f0 (Accessed 2024-03-31).

#### EC 1 data from:

Pennington S C ; Alford S ; Back M P ; Bailey V ; Baldwin A ; Bolinger J ; Bowe M ; Boyanov M I ; Cianci-Gaskill J A ; Conroy N A ; Cooper M J ; Day D ; Demeo A ; Derby K ; Detweiler D ; Devres-Zimmerman S ; Eberhard E ; Gedan K ; Haaf L ; Homolka K K ; Johnson E ; Kemner K M ; Khan A ; Kirwan M ; Kittaka P ; Koontz E ; Langley A ; Leff R ; Lerberg S ; Lewis A M ; Malkin S ; Marcarelli A M ; McMurray S E ; Messerschmidt T ; Michael T C ; Michael H A ; Minor E C ; Moye B ; Mozdzer T J ; Neubauer S ; Norris C G ; O'Loughlin E J ; Otenburg O ; Pain A ; Patel K F ; Philben M ; Phillips E ; Pratt D ; Regier P ; Jr J A R ; Sage L ; Sandborn D ; Smith S ; Smith A ; Soin-Voshell S ; Song B ; Sprague-Getsy A ; Laurent K S ; Staver L ; Stearns A ; Stetten L ; Swerida R ; Theuerkauf E J ; Tully K ; Vargas R ; Ward N D ; Watson E ; Weilminster C ; Myers-Pigg A N (2023): EXCHANGE Campaign 1: A Community-Driven Baseline Characterization of Soils, Sediments, and Water Across Coastal Gradients. COMPASS-FME, ESS-DIVE repository. Dataset. doi:10.15485/1960313 accessed via https://data.ess-dive.lbl.gov/datasets/doi:10.15485/1960313 on 2024-03-31

```{r data loading, warning=FALSE, echo=FALSE}
#vertical Forest Retreat Rate
vfrr_shp <- st_read("../data_do_not_commit/knb-lter-vcr.399.1/Vertical_forest_retreat_rate/Vertical_forest_retreat_rate.shp")

#EC1 Metadata
ec1_meta <-read_csv("../data_do_not_commit/EC1 Package V1/ec1_metadata_v1/ec1_metadata_collectionlevel.csv")

ec1_meta_kit <-read_csv("../data_do_not_commit/EC1 Package V1/ec1_metadata_v1/ec1_metadata_kitlevel.csv")

#Soil Data
ec1_bd <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_soil_v1/ec1_soil_bulk_density_L2.csv")
ec1_cond <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_soil_v1/ec1_soil_cond_L2.csv")
ec1_gwc <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_soil_v1/ec1_soil_gwc_L2.csv")
ec1_ph <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_soil_v1/ec1_soil_ph_L2.csv")
ec1_tc <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_soil_v1/ec1_soil_tc_L2.csv")
ec1_tn <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_soil_v1/ec1_soil_tn_L2.csv")

#adjacent surface water data:
ec1_water_sal <- read_csv("../data_do_not_commit/EC1 Package V1/ec1_water_v1/ec1_water_salinity_L2.csv") %>%
  rename(surface_water_sal_psu = sal_psu) %>%
  dplyr::select(kit_id, surface_water_sal_psu)

```
```{r cleaning data, warnings=FALSE, echo=FALSE}
ec1_meta_curated <- ec1_meta %>%
  full_join(ec1_meta_kit, by="kit_id") %>%
  filter(region == "Mid-Atlantic") %>%
  #can't do this analysis on the wetland, sediment or water data
  #Want to look for the variables for the upland, though
  filter(transect_location %in% c("upland")) %>%
  filter(!is.na(latitude)) %>%
  dplyr::select(kit_id, region, transect_location, site_name, latitude, longitude, elevation_m, soil_type, soil_horizon)

ec1_soil_all <- ec1_bd %>%
  full_join(ec1_cond, by=c("campaign", "kit_id", "transect_location")) %>%
  full_join(ec1_gwc, by=c("campaign", "kit_id", "transect_location")) %>%
  full_join(ec1_ph, by=c("campaign", "kit_id", "transect_location")) %>%
  full_join(ec1_tc, by=c("campaign", "kit_id", "transect_location")) %>%
  full_join(ec1_tn, by=c("campaign", "kit_id", "transect_location")) %>%
  inner_join(ec1_meta_curated, by=c("kit_id","transect_location"))
```

Projected Coordinate Reference System (CRS) "WGS 84 / UTM zone 18N" = EPSG code for this CRS is 32618.

```{r data extracting, echo=FALSE}

# Want to focus on the transition sites for lat and long: 
coordinates <- ec1_meta_curated %>%
  dplyr::select(kit_id, transect_location, longitude, latitude) %>% 
   filter(!(kit_id == "K055" & transect_location == "upland"),
          !(kit_id == "K058" & transect_location == "upland"),
          !(kit_id == "K042" & transect_location == "upland"),
  #        !(kit_id == "K042" & transect_location == "transition"),
          !(kit_id == "K045" & transect_location == "upland"),
  #        !(kit_id == "K045" & transect_location == "transition"),
          !(kit_id == "K048" & transect_location == "upland"),
  #        !(kit_id == "K048" & transect_location == "transition")
  ) #no retreat data here

coordinates_sf <- st_as_sf(coordinates, coords = c("longitude", "latitude"), crs = 4326)

# Transform the CRS to match the shapefile
coordinates_sf_transformed <- st_transform(coordinates_sf, 32618)

extracted_vfrr_data <- st_join(coordinates_sf_transformed, vfrr_shp, join = st_intersects)

extracted_vfrr_no_geometry <- st_set_geometry(extracted_vfrr_data, NULL)

extracted_vfrr_df <- as.data.frame(extracted_vfrr_no_geometry) %>%
  #filter out the -9999 values
  filter(V_RetreatR > 0 & V_RetreatR < 99) %>%
  #filter out places with NAs
  filter(!is.na(V_RetreatR))

```

```{r data merging, echo=FALSE}

clc_all_data <- extracted_vfrr_df %>%
  inner_join(ec1_soil_all, by= c("kit_id","transect_location")) %>%
  left_join(ec1_water_sal, by = "kit_id")
```

# Analysis time! 
vertical retreat rate   (m year−1)

```{r correlations, echo=FALSE}
clc_data_num <- clc_all_data %>% 
  dplyr::select(where(is.numeric)) 

#linear, parametric 
pearsons <- clc_data_num %>%
  correlate(use = "pairwise.complete.obs", method = "pearson") %>% 
  focus(V_RetreatR)

print(pearsons)

lm_clc <- lm(V_RetreatR ~ ., data= clc_data_num)
summary(lm_clc)

#non-parametric
spearmans <- clc_data_num %>%
  correlate(use = "pairwise.complete.obs", method = "spearman") %>% 
  focus(V_RetreatR)

print(spearmans)

```
Looking maybe promising? Need significance values though...

```{r sig or not, echo=FALSE}

results <- sapply(clc_data_num[-1], function(x) {
  test <- cor.test(clc_data_num$V_RetreatR, x)
  c(correlation = test$estimate, p.value = test$p.value)
})

transposed <- t(results)

pearsons_stats_a <- as.data.frame(transposed)

# significance as p < 0.05
pearsons_stats <- pearsons_stats_a %>%
  rownames_to_column(var = "Variable") %>%
  filter(p.value < 0.1)

print(pearsons_stats)
```

nope, nothing...dang

## Plotting 
```{r plotting sign values, echo=FALSE}

gwc_corrplot <- clc_all_data  %>%
  mutate_if(is.character, funs(ifelse(is.na(.), "Unknown", .))) %>%
  ggplot(aes(x=moisturecontent_perc_drywtbasis, y=V_RetreatR, color= soil_horizon)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, color = "black")+
  theme_classic()

gwc_corrplot

cowplot::save_plot("../graphs/gwc_correlation_plot.jpeg", gwc_corrplot, dpi=300)

```

Hmm, wondering if I have enough data to try random forest? Probably not going to work, messing around with it anyway...
```{r random forest trying out, include =FALSE}
#Need to get rid of NA values first.
#filling these with mean of the column, for now. 

clc_4rf <-clc_all_data %>% 
  dplyr::select(-site_name, - kit_id, -latitude, -longitude, -region, -campaign, -transect_location, -carbon_weight_perc, -nitrogen_weight_perc, -ph) %>%
  mutate_if(is.character, funs(ifelse(is.na(.), "Unknown", .))) %>%
  mutate_if(is.numeric, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)
  )

# Set up the random forest model
clc_rf <- ranger(V_RetreatR ~ ., data= clc_4rf, importance = 'permutation')

# Print the model
print(clc_rf)

```
messing around some more...
```{r more random forest testing, include = FALSE}

# Get variable importance
importance <- clc_rf$variable.importance

# Convert to data frame
importance_df <- data.frame(Variable = names(importance), Importance = importance)

# Order by importance
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot
VI <- importance_df %>%
  ggplot() +
  geom_bar(aes(x = reorder(Variable, Importance), y = Importance), stat = "identity") +
  coord_flip() +
  labs(x = "Variable", y = "Importance", title = "Variable Importance") +
  theme_bw()

```

```{r print rf}
VI
```

# Make a map

```{r mapping, echo=FALSE}
pal <- wesanderson::wes_palette("Zissou1", 100, type = "continuous")

usa <- maps::map('usa', fill = TRUE, plot = FALSE)
usa <- sf::st_as_sf(usa)
usa_transf <-st_transform(usa, 32618)

coordinates_df <- st_coordinates(coordinates_sf_transformed) %>% 
  as.data.frame() %>% 
  rename(lon = X, lat = Y)

vfrr_shp_corrected <- vfrr_shp %>% 
  filter(V_RetreatR > 0 & V_RetreatR < 5) 

bbox <- st_bbox(vfrr_shp_corrected)

map <- ggplot() +
  geom_sf(data = usa, fill = "#749B3A", color = "transparent", alpha=0.05) +
  geom_sf(data= vfrr_shp_corrected, aes(color=V_RetreatR), size=3) +
   scale_color_gradientn(colors=pal)+
  geom_point(data= coordinates_df, aes(x = lon, y = lat), color="black", fill=NA, size=1, shape=1 ) +
 #   geom_sf(data= coordinates_sf_transformed, aes(geometry= geometry), color="black", fill=NA,size=1,shape=1 )+
  coord_sf(xlim = c(bbox["xmin"], bbox["xmax"]), ylim = c(bbox["ymin"], bbox["ymax"]), crs= 32618)+
  theme_bw() + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
   labs(color ="vertical \nretreat rate \n(mm/year)", x =" ", y=" ")

cowplot::save_plot("../graphs/map_V2.jpeg", map, dpi=300)

map

```

## Threshold Elevations

The second dataset is the threshold elevation defining the boundary between forest and marsh, aggregated by watershed. This is only for Maryland and Virginia, so won’t work for EXCHANGE sites in DE or NJ.
https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-vcr.343.2
File #1 contains the watershed names and threshold elevations. File #3 contains the watershed boundaries. You’ll need to identify the watershed that the EXCHANGE site is in, then find the threshold elevation for that watershed before running the regression. Its simpler than it sounds.

```{r load in watersheds data, echo=FALSE}

elevation_df <- read_csv("../data_do_not_commit/knb-lter-vcr.343.2/CB_landuse.csv", skip = 23)

HUC_shp <- st_read("../data_do_not_commit/knb-lter-vcr.343.2/CB_HUC10/CB_HUC10.shp")
```


Projected CRS: NAD83 / UTM zone 18N = crs epsg 26918 aka should be the same as  st_crs(HUC_shp)
```{r data extracting in other crs, echo=FALSE}

ec1_meta_curated_2 <- ec1_meta %>%
  full_join(ec1_meta_kit, by="kit_id") %>%
  filter(region == "Mid-Atlantic") %>%
  #can't do this analysis on the wetland, sediment or water data
  filter(transect_location %in% c("upland")) %>%
  filter(!is.na(latitude)) %>%
  dplyr::select(kit_id, region, transect_location, site_name, latitude, longitude, elevation_m, soil_type, soil_horizon)

coordinates_ws <- ec1_meta_curated_2 %>%
  dplyr::select(kit_id, transect_location, longitude, latitude) 

coordinates_sf_1 <- st_as_sf(coordinates_ws, coords = c("longitude", "latitude"), crs = 4326)

# Transform the CRS to match the shapefile
coordinates_ws_sf_transformed <- st_transform(coordinates_sf_1, 26918)


points_in_watershed <- st_join(coordinates_ws_sf_transformed, HUC_shp, join = st_within) %>%
  drop_na()



```

```{r data merging elevation, echo=FALSE}

points_in_watershed_no_geometry <- st_set_geometry(points_in_watershed, NULL)

clc_all_data_ws <- points_in_watershed_no_geometry %>%
  inner_join(ec1_soil_all, by= c("kit_id", "transect_location")) %>%
  left_join(ec1_water_sal, by = "kit_id")
```

# Analysis time! 
## Elevation 

```{r correlations elevation, echo=FALSE}


clc_data_num_2 <- clc_all_data_ws %>% 
  dplyr::select(where(is.numeric)) 

#linear, parametric 
pearsons1 <- clc_data_num_2%>%
  correlate(use = "pairwise.complete.obs", method = "pearson") %>% 
  focus(Elevation)

print(pearsons1)

lm_clc1 <- lm(Elevation ~ ., data= clc_data_num_2)
summary(lm_clc1)

#non-parametric
spearmans1 <- clc_data_num_2 %>%
  correlate(use = "pairwise.complete.obs", method = "spearman") %>% 
  focus(Elevation)

print(spearmans1)

```

```{r sig or not Take 2, echo=FALSE}

results2 <- sapply(clc_data_num_2[-1], function(x) {
  test <- cor.test(clc_data_num_2$Elevation, x)
  c(correlation = test$estimate, p.value = test$p.value)
})

transposed2 <- t(results2)

pearsons_stats_a2 <- as.data.frame(transposed2)

# significance as p < 0.05
pearsons_stats2 <- pearsons_stats_a2 %>%
  rownames_to_column(var = "Variable") %>%
  filter(p.value < 0.05)

print(pearsons_stats2)
```


```{r plotting elevation, echo=FALSE}
gwc_corrplot <- clc_all_data_ws  %>%
  mutate_if(is.character, funs(ifelse(is.na(.), "Unknown", .))) %>%
  ggplot(aes(x=moisturecontent_perc_drywtbasis, y=Elevation)) +
  geom_point(color = "#749B3A", size = 3) +
  geom_smooth(method = "lm", se = TRUE,  color = "black")+
  theme_classic()+
   theme(legend.position="none",
            axis.text.y=element_text(size=14, color="black"),
              axis.title.x=element_text(size=14, color="black"),
              axis.title.y=element_text(size=14),
              axis.text.x=element_text(size=14, color="black"))+
  labs(x ="Moisture Content in upland soil (% dry weight)", y="Threshold Elevation (m)")

gwc_corrplot

cowplot::save_plot("../graphs/gwc_correlation_plot_threshold_uplands.jpeg", gwc_corrplot, dpi=300)
```


```{r plotting sal, echo=FALSE}
sal_corrplot <- clc_all_data_ws  %>%
  mutate_if(is.character, funs(ifelse(is.na(.), "Unknown", .))) %>%
  ggplot(aes(x=surface_water_sal_psu, y=Elevation)) +
  geom_point(color = "#749B3A") +
  geom_smooth(method = "lm", se = TRUE,  color = "black")+
  theme_classic()+
  labs(x ="Adjacent Surface Water Salinity (PSU)", y="Threshold Elevation (m)")

sal_corrplot

cowplot::save_plot("../graphs/surface_sal_plot_threshold_upland.jpeg", sal_corrplot, dpi=300)
```

Hmm, wondering if I have enough data to try random forest? Probably not going to work, messing around with it anyway...
```{r random forest trying out, include =FALSE}
#Need to get rid of NA values first.
#filling these with mean of the column, for now. 

clc_4rf_el <-clc_all_data_ws %>% 
  dplyr::select(-site_name, -name, - kit_id, -latitude, -longitude, -region, -campaign, -transect_location, -carbon_weight_perc, -nitrogen_weight_perc, -ph) %>%
  mutate_if(is.character, funs(ifelse(is.na(.), "Unknown", .))) %>%
  mutate_if(is.numeric, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)
  ) %>%
  rename("dependent" = Elevation) %>%
  mutate(white_noise = rnorm(1:n(), mean = 0, sd = 1))


# Set up the random forest model
clc_rf_el <- ranger(dependent ~ ., data= clc_4rf_el, importance = 'permutation')

# Print the model
print(clc_rf_el)

plot_fi <- function(data){
  
  model_recipe <- data %>% 
    recipe(dependent ~ .) %>% 
    step_integer(soil_type) %>% 
    step_integer(soil_horizon) %>% 
    step_corr(all_predictors()) %>% 
    step_normalize(all_predictors(), -all_outcomes()) %>% 
    recipes::prep()
  
  df <- model_recipe %>% 
    bake(data)
  
  ## Make model
  rf_model <- ranger(dependent ~ ., data = df, importance = "impurity")
  
  ## Visualize model output
  print(rf_model)
  
  ## Set vectors for renaming stuff
  var_names <- rf_model$variable.importance
  col_names <- c("predictor", "raw_fi")
  
  ## Convert feature importance to a tibble with variables as a column
  fi0 <- as.data.frame(var_names) %>% 
    tibble::rownames_to_column() %>% 
    as_tibble()
  
  ## Rename columns
  colnames(fi0) = col_names
  
  ## Output variable importance (or feature importance)
  fi0
}

clc_rf_el_norm <- plot_fi(clc_4rf_el) 

```

## Function to calculate variable importance for a default ranger() model
[10:26 AM] Regier, Peter J
I'd say if you're using it as an initial diagnostic, plot up the feature importance and use that. One other trick I'd suggest is add white noise as a predictor, then only include variables that are above white noise. That will probably help your MLR, because that correlation call removes variables that are super strongly correlated which is good for MLR and the white noise gives you a justifiable threshold
 
  
messing around some more...
```{r more random forest testing, include = FALSE}

# Get variable importance
importance_el <- clc_rf_el_norm$variable.importance

# Convert to data frame
importance_df_el <- data.frame(Variable = names(importance_el), Importance = importance_el)

# Order by importance
importance_df_el <- importance_df_el[order(importance_df$Importance, decreasing = TRUE), ]

# Plot
VI_el <- importance_df_el %>%
  ggplot() +
  geom_bar(aes(x = reorder(Variable, Importance), y = Importance, fill = Variable), stat = "identity") +
  coord_flip() +
  labs(x = "Variable", y = "Importance", title = "Variable Importance") +
  theme_bw()

```

```{r print rf}
VI_el

cowplot::save_plot("../graphs/variable_imp_RF_threshold_upland.jpeg", VI_el, dpi=300)

```
